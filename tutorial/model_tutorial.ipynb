{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model tutorial\n",
    "Michael Chimento\n",
    "## Table of Contents\n",
    "\n",
    "* [Introduction](#introduction)\n",
    "* [Behaviors](#behaviors)\n",
    "* [Agents](#agents)\n",
    "    * [Instance variables: knowledge state, memory and parameter values](#agents_1)\n",
    "    * [Class methods: observation and memory](#agents_2)\n",
    "    * [Class methods: EWA functions](#agents_3)\n",
    "* [NBDA](#NBDA)\n",
    "* [Networks](#networks)\n",
    "* [Simulation](#simulation)\n",
    "    * [Helper functions](#helper-agent)\n",
    "    * [Helper functions: turnover](#helper-turnover)\n",
    "    * [Helper functions: extracting data](#helper-extract)\n",
    "* [Running the simulation](#running)\n",
    "    * [Simulation parameters](#sim_params)\n",
    "    * [Population parameters](#pop_params)\n",
    "    * [EWA parameters](#EWA_params)\n",
    "    * [NBDA parameters](#NBDA_params)\n",
    "    * [Execution](#run_sim)\n",
    "    * [Data management](#data_mgmt)\n",
    "\n",
    "## Introduction <a class=\"anchor\" id=\"introduction\"></a>\n",
    "\n",
    "This companion document is meant to show how we have implemented our agent based model in Python3. We used an object oriented programming style to increase the flexibility of the model, and make life easier when the model becomes complicated or is extended.\n",
    "\n",
    "First we load the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "from collections import Counter\n",
    "import math\n",
    "import sys\n",
    "import networkx as nx\n",
    "import random\n",
    "import numpy as np\n",
    "from os.path import isfile,join\n",
    "import pandas as pd\n",
    "from os import listdir,remove\n",
    "import time\n",
    "from scipy.special import logsumexp\n",
    "import jdc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behaviors <a class=\"anchor\" id=\"behaviors\"></a>\n",
    "\n",
    "Let's first define a class called `behavior`. Each instance of `behavior` will represent a possible behavior that agents will be able to learn during the simulation. Any python class can be given its own variables and methods using `self.var_name` convention. Here, we define a initialization method that assigns each instance of a behavior with a name, an associated payoff, and a base learning rate. The payoff defines the reward obtained for producing the behavior, and the base rate is used in the NBDA equation that determines whether an agent will acquire the behaviour at any given timestep. The base hazard represents the likelihood of acquiring the behavior at each timestep which is independent of any social or asocial influence. A higher base base value means that the behavior will be easier to acquire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the class behavior\n",
    "class behavior:\n",
    "    def __init__(self,name,payoff,base_rate):\n",
    "        self.name = name #string name of behavior\n",
    "        self.payoff = payoff #float payoff of behavior\n",
    "        self.base_rate = base_rate #baseline learning parameter (lambda_0) from NBDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents <a class=\"anchor\" id=\"agents\"></a>\n",
    "\n",
    "Next, let's define the `agent` class. During a simulation, each node in our network will be occupied by an instance of this class.\n",
    "\n",
    "### Instance variables: knowledge state, memory and parameter values <a class=\"anchor\" id=\"agents_1\"></a>\n",
    "We will initialize agents with a few different instance variables, including:\n",
    "\n",
    "* `self.id` a unique ID, \n",
    "* `self.knowledge` a dictionary that will hold the behaviors that the agent knows how to produce. Keys are the behavior names, and values are the associated information with each behavior, including its attraction score, individual and social weights for behavior, the final probability of producing the behavior.\n",
    "* `self.ind_memory` a list that will hold a history of behaviors that they've produced themselves within the memory window\n",
    "* `self.temp_social_memory` a list that is a short-term social memory, which holds all the behaviors that an agent has observed its neighbors produce in the current time step\n",
    "* `self.long_social_memory` a list that is a long-term social memory, which holds all the behaviors that an agent has observed its neighbors produce within the memory window.\n",
    "* `self.exposure` represents how many timesteps the agent has been in the simulation, or exposed to the simulation environment. This will be later used for informing a population turnover function.\n",
    "* `self.naive` a boolean value that represents whether an agent knows any behaviors at all\n",
    "\n",
    "Each agent is also initialized with some parameter values that will affect it's behavioral productions. These parameter values can vary from agent to agent, and can even be redefined using a function, all depending on the requirements of the simulation. This allows for flexible definitions of population heterogeneity. \n",
    "* `self.sigma` the agent's social information bias parameter (used in EWA)\n",
    "* `self.phi` the agent's recent information bias parameter (used in EWA)\n",
    "* `self.f_SI` the agent's conformity exponent (used in EWA)\n",
    "* `self.tau` the agent's sensitivity to differences in attraction scores (used in EWA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent:\n",
    "    def __init__(self,ID):\n",
    "        self.id = ID #keeps agents identifiable across simulations\n",
    "        self.knowledge = {} #name, a_mat, i_mat, s_mat, p_mat,solve_count\n",
    "        self.ind_memory = [] #records agent's own productions for the duration of the memory window\n",
    "        self.temp_social_memory = [] #records observed productions for the duration of the timestep\n",
    "        self.long_social_memory = [] #records lists of lists, each sublist from a single timestep within the memory window\n",
    "        self.naive = True # binary variable that flips to False once any behavior has been learned by the agent\n",
    "\n",
    "        #EWA parameters\n",
    "        self.sigma = sigma #social information bias\n",
    "        self.phi = phi #recent payoff bias\n",
    "        self.f_SI = f_SI #conformity of social influence exponent\n",
    "        self.tau = tau #inverse exploration parameter (conservatism)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class methods: observation and memory <a class=\"anchor\" id=\"agents_2\"></a>\n",
    "\n",
    "The following class methods handle aspects of the agents memory of their own productions, and their social productions.\n",
    "\n",
    "* `observe(behavior)` is called after each behavior is produced. An agent may only observe it's neighbors, and the observed behavior is added to its short term social memory.\n",
    "* `consolidate_social_memory()` is called once per timestep, after all agents have produced their behaviors. It adds the memory from the current timestep to the long term social memory.\n",
    "* `prune_ind_memory(timestep)` is called once per timestep and, using a first in, first out (FIFO) rule, removes any behaviors from an agent's individual memory that were produced outside of the memory window.\n",
    "* `prune_social_memory(timestep)` is called once per timestep and, using a FIFO rule, removes any behaviors from an agent's social memory that were produced outside of the memory window.\n",
    "* `reset_temp_social_memory()` erases all observed behaviours from the previous timestep in preparation for the next set of observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to agent\n",
    "\n",
    "def observe(self,behavior):\n",
    "        if behavior in self.knowledge:\n",
    "            self.temp_social_memory.append(behavior)\n",
    "            #print(\"observation by {}: {}\".format(self.id, self.knowledge[behavior]))\n",
    "\n",
    "def prune_ind_memory(self):\n",
    "    while len(self.ind_memory) > memory_window:\n",
    "        self.ind_memory.pop(0)\n",
    "    #assert len(self.ind_memory) <= memory_window, print(\"Individual memory has exceeded memory window.\",len(self.ind_memory))\n",
    "\n",
    "def prune_social_memory(self):\n",
    "    while len(self.long_social_memory) > memory_window:\n",
    "        self.long_social_memory.pop(0)\n",
    "    #assert len(self.long_social_memory) <= memory_window, print(\"Social memory has exceeded memory window.\",len(self.long_social_memory))\n",
    "\n",
    "def consolidate_social_memory(self):\n",
    "    self.long_social_memory.append(self.temp_social_memory)\n",
    "\n",
    "def reset_temp_social_memory(self):\n",
    "    self.temp_social_memory = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class methods: EWA functions <a class=\"anchor\" id=\"agents_3\"></a>\n",
    "\n",
    "Each agent contains a method that calculate an attraction score from received behavioral payoffs. This attraction score is turned into an individual weight using a softmax function. The agents also keep track of what behaviors their neighbors produce within the memory window. These values are turned into a social weight. Finally, the individual and social weights are combined into a final probability that the agents would produce a given behavior. This is summarized in the steps below.\n",
    "\n",
    "`A_mat_update(behavior)` updates an agent's attraction score matrix using the following equation: $A_{kt} = \\phi\\pi_k + (1-\\phi)A_{k,t-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to agent\n",
    "def A_mat_update(self, produced_behavior):\n",
    "    reward = all_behaviors[produced_behavior].payoff\n",
    "    for known_behavior in self.knowledge.keys():\n",
    "        if known_behavior==produced_behavior:\n",
    "            new_A_kt = (1 - self.phi)*self.knowledge[known_behavior][\"a_mat\"] + self.phi*reward\n",
    "        else:\n",
    "            new_A_kt = (1 - self.phi)*self.knowledge[known_behavior][\"a_mat\"] #null payoff for non-produced behavior\n",
    "        self.knowledge[known_behavior][\"a_mat\"] = new_A_kt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`I_mat_update(behavior)` updates an agent's individual weight matrix using the following equation: $I_{kt} = \\frac{exp(\\tau A_{kt})}{\\sum_k^Z{exp(\\tau A_{kt})}}$. The use of the softmax equation here guarantees that the sum of all values for known behaviors is 1. To avoid overflow or underflow errors, we have calculated $I_kt$ in log space, and will exponentiate it in `P_mat_update`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to agent\n",
    "def I_mat_update(self):\n",
    "    if not self.naive:\n",
    "        A_mat = np.array([behavior[\"a_mat\"] for behavior in self.knowledge.values()])\n",
    "        tau_by_A_mat = np.multiply(A_mat, self.tau)\n",
    "        summed_A_mat = logsumexp(tau_by_A_mat)\n",
    "        for count, behavior in enumerate(self.knowledge.values()):\n",
    "            behavior[\"i_mat\"] = tau_by_A_mat[count] - summed_A_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`S_mat_update(behavior)` updates an agent's social weight matrix using the following equation: $S_{kt} = \\frac{(\\sum_{t}^{m}{n_{kt}})^{f_{SI}}}{\\sum_k^{Z}(\\sum_t^m{n_{kt}})^{f_{SI}}}$. This equation ensures that the values for each known behavior are bounded between 0 and 1. First the long term memory is flattened. $n$ values are determined using the `count()` function on the long term memory list. The denominator is calculated first to save later computation, and then each new social weight matrix value is calculated for known behaviors, and written into the knowledge dictionary under that behavior key's `[\"s_mat\"]` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to agent\n",
    "def S_mat_update(self):\n",
    "    if not self.naive:\n",
    "        social_memory = [item for subl in self.long_social_memory for item in subl]\n",
    "        assert len(social_memory) <= (N * memory_window), print(\"memory {} exceeds memory window {}\".format(len(social_memory),N*memory_window))\n",
    "        if len(social_memory)>0:\n",
    "            denom=0\n",
    "            for behavior in self.knowledge.keys():\n",
    "                denom += social_memory.count(behavior)**self.f_SI\n",
    "            for behavior in self.knowledge.keys():\n",
    "                new_S_kt = (social_memory.count(behavior)**self.f_SI) / denom\n",
    "                self.knowledge[behavior][\"s_mat\"] = new_S_kt\n",
    "\n",
    "        #if no social memory present\n",
    "        else:\n",
    "            for count,behavior in enumerate(self.knowledge.values()):\n",
    "                new_S_kt = 0\n",
    "                behavior[\"s_mat\"] = new_S_kt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`P_mat_update(behavior)` updates an agent's probability matrix using the following equation: $P_{kt} = (1-\\sigma)I_{kt} + \\sigma S_{kt}$. If an agent has no experience observing others, the value from their individual weight matrix is the final production probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to agent\n",
    "def P_mat_update(self):\n",
    "    if not self.naive:\n",
    "        social_memory = np.array([x[\"s_mat\"] for x in self.knowledge.values()])\n",
    "        if np.sum(social_memory)>0:\n",
    "            for behavior in self.knowledge.values():\n",
    "                behavior[\"p_mat\"] = (1 - self.sigma)*np.exp(behavior[\"i_mat\"]) + self.sigma*behavior[\"s_mat\"]\n",
    "\n",
    "        else:\n",
    "            for behavior in self.knowledge.values():\n",
    "                behavior[\"p_mat\"] = np.exp(behavior[\"i_mat\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need the agents to do something with the knowledge they have. They can produce behaviors using the following method. This creates a list of possible behaviors from their knowledge dictionary keys. One behavior is chosen from using Numpy's `random.choice()` function, with the probability values passed as the probability argument to weight the choice.\n",
    "\n",
    "That behavior is then added to the agent's individual memory, and is passed to `A_mat_update()` to update attraction, then individual weight, and finally probability (in case the simulation allows for more than one behavioral production per timestep). The name of the behavior produced is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to agent\n",
    "def produceBehavior(self):\n",
    "    behavior_choices = [behavior for behavior in self.knowledge.keys()]\n",
    "    p_mat = np.array([x[\"p_mat\"] for x in self.knowledge.values()])\n",
    "\n",
    "\n",
    "    #choose production with probability from p_mat\n",
    "    try:\n",
    "        production = np.random.choice(behavior_choices, p=p_mat)\n",
    "    except Exception as e:\n",
    "        print(\"Error producing behavior:\", e)\n",
    "        print(sum(p_mat))\n",
    "\n",
    "\n",
    "    self.ind_memory.append(production) #adds production to memory\n",
    "\n",
    "    self.A_mat_update(production) #update attraction score for next production\n",
    "\n",
    "    return production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final method definition in the agent class is `acquire_behavior(G)` which takes the social network object G as an argument, is run once per timestep, and determines whether agents learn a behavior in a given timestep. First, it creates a list of neighboring nodes of the focal agent. Then, for every behavior that's not known to the focal agent, it calculates the probability of acquisition. This requires the use of the NBDA equation, which takes into account information about the agent's neighbors. This will be covered in the next section. Then, a random number between $[0,1)$ is generated, and if this number is smaller than the acquisition probability, the focal agent acquires the behavior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to agent\n",
    "def acquire_behavior(self, G):\n",
    "    neighbors = [node for node in G.neighbors(self.id)]\n",
    "    for behavior.name in all_behaviors:\n",
    "        if behavior.name not in self.knowledge.keys():\n",
    "            acquisition_prob = lambda_t(behavior.name,neighbors,G)\n",
    "            assert 0 <= acquisition_prob <=1, \"resulting acquision prob from NBDA must be between 0 and 1\"\n",
    "            if random.random() <= acquisition_prob:\n",
    "                self.knowledge[behavior.name] = {\"a_mat\": 0,\"i_mat\": 0,\"s_mat\":0,\"p_mat\":0}\n",
    "                self.I_mat_update()\n",
    "                self.S_mat_update()\n",
    "                self.P_mat_update()\n",
    "                self.naive=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NBDA <a class=\"anchor\" id=\"NBDA\"></a>\n",
    "\n",
    "While the production probabilities are handled by methods in the `agent` class, the calculation of acquisition probabilities by NBDA equations is handled by the following modular series of functions. The basic form of NBDA is given by $\\lambda_i(t) = \\lambda_0(t)(T(a_i,z(t))+A)(1-z_i(t))$.\n",
    "\n",
    "* $\\lambda_{ik}(t)$ - the rate of transmission for individual $i$ at time $t$ for behavior $k$.\n",
    "* $ \\lambda_{0k}(t)$ - the baseline rate of transmission at time $t$ for behavior $k$\n",
    "* $(T(a_i,z_k(t))) = s\\sum{a_{ij}z_{jk}(t)}$ - the transmission function, which can be represented by a variety of forms accommodating simple or complex contagion. This is composed of:\n",
    " + $s$ - the rate of social acquisition per unit connection, and can take values from $[0,\\infty]$\n",
    " + $a_i$ - the association matrix for the focal individual representing all connections to neighbors\n",
    " + $z_jk(t) = \\frac{n_{kjt}}{\\sum{n_{kjt}}}$ - In the FSSL model, this takes a value between 0 or 1, such that it includes information about the production frequency of the behavior within the memory window. NB: in traditional NBDA, this is a binary variable, representing the knowledge state of the neighbor.\n",
    "* $A$ - is the presence of asocial learning, in this version of the model it's binary, but could be further defined by ILVs\n",
    "* $(1-z_{ik}(t))$ ensures the equation equals zero if the behavior is already known.\n",
    "\n",
    "We will cover how this is implemented in detail below.\n",
    "\n",
    "First, we have a convenience function that is called within the simulation, once per timestep, that loops through all agents and calls the `acquire_behavior()` method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceeding code section for the agent method `acquire_behavior()`, `acquisition_prob` requires a call to `lambda_t()`. This function is at the \"top\" level of NBDA, and first calculates the value for $T(a_i,z_k(t)))$, $A$, and with those then $\\lambda_i(t)$. That value is a rate, which is converted into a probability using $P(acquire)=1-exp(-\\lambda_i(t))$. This probability is returned for further use in `acquire_behavior()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_t(behavior, neighbors, G):\n",
    "    transmission_func = transmission_function(behavior, neighbors, G)\n",
    "\n",
    "    asocial_learning_func = A_param()\n",
    "\n",
    "    #lambda(t) = baseline rate function*(transmission_func + ind_learning_parameters)\n",
    "    acq_rate = all_behaviors[behavior].base_rate * (transmission_func + asocial_learning_func)\n",
    "\n",
    "    #convert from rate to probability within the last timestep that an agent has acquired the behavior\n",
    "    acq_prob = 1-math.exp( -acq_rate )\n",
    "\n",
    "    return acq_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `transmission_function()` takes information about the knowledge states of neighbors and multiplies it by the $s$ parameter, which represents the strength of social learning. Here, we have implemented several forms which this function could take, whose definitions are taken from (Firth et al., 2020).\n",
    "\n",
    "* `NBDA_type=0` the \"default\" NBDA, in which $s$ is unbounded and $z_{jk}(t)$ is passed as its raw form.\n",
    "* `NBDA_type=1` NBDA in which $s$ and $A$ are linked ($A=1-s$) and bounded between $[0,1]$.\n",
    "* `NBDA_type=2` \"proportional rule\" NBDA, $z_{jk}(t)$ is normalized by the total number of neighbors.\n",
    "* `NBDA_type=3` \"frequency dependent rule\" NBDA, $z_{jk}(t)$ is exponentiated by a conformity exponent, in the same manner as in the EWA equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transmission_function(behavior,neighbors,G):\n",
    "    \n",
    "    z_jt_param = z_jt(behavior, neighbors, G)\n",
    "    \n",
    "    #Hoppitt_Laland 2013 General form, s is unbounded\n",
    "    #lambda(t) = baseline rate function*(rate of acquisition * sum(a_ij*z_j(t)) + A\n",
    "    if NBDA_type == 0 or NBDA_type == 1:\n",
    "        transmission_func = s_param * z_jt_param\n",
    "\n",
    "    #Proportional rule model Firth 2020\n",
    "    elif NBDA_type == 2:\n",
    "        assert z_jt_type == \"binary\", \"z_jt must be binary to appropriately calculate NBDA type 2 (proportional rule)\"\n",
    "        transmission_func = s_param * (z_jt_param / len(neighbors))\n",
    "\n",
    "    #Freq dependent rule model Firth 2020\n",
    "    elif NBDA_type == 3:\n",
    "        #assert z_jt_type==\"binary\", \"z_jt must be binary to appropriately calculate NBDA type 3 (conformity rule)\"\n",
    "        numerator = z_jt_param**f_SL\n",
    "        denominator = z_jt_param**f_SL + ((len(neighbors)-z_jt_param) ** f_SL)\n",
    "        transmission_func = s_param * (numerator/denominator)\n",
    "        \n",
    "    return transmission_func\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transmission function first makes a call to the `z_jt` function to get its value. The form is determined by the simulation variable `NBDA_z_jt_type`. The standard NBDA is `binary`, where a knowledgeable neighbor counts as +1. Setting this in FSSL decouples EWA from NBDA, and is included here for the purpose of comparison of simulations in which the two models are not allowed to inform each other. FSSL is meant to run with type `proportional`, in which information about how frequently the neighbor produced the behavior is included in their contribution to the value of $z_{jk}(t)$. This is done by dividing the number of behavioral productions of a given behavior by the total number of behaviors produced, such that a neighbors contribution can range from $[0,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_jt(behavior, neighbors, G):\n",
    "    z_jt = 0\n",
    "\n",
    "    for neighbor in neighbors:\n",
    "        if behavior in G.nodes[neighbor][\"data\"].knowledge.keys():\n",
    "            #Values range integers between [0,length(neighbors)]\n",
    "            if z_jt_type==\"binary\":\n",
    "                z_jt += 1\n",
    "            #Values range continuously from [0,length(neighbors)] with each neighbor weighted in proportion to solution type\n",
    "            elif z_jt_type==\"proportional\":\n",
    "            \t#how many times a neighbor produces the behavior, over the total # of behaviors produced within the memory window\n",
    "                z_jt += G.nodes[neighbor][\"data\"].ind_memory.count(behavior)/memory_window\n",
    "\n",
    "    return z_jt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For future extensibility, the value of $A$ is defined as a function. This function could eventually make use of ILVs to calculate it's value. In this version, the function first checks the value of simulation variable `asocial_learning`. If this is false, $A=0$. If this is true and the NBDA is of the bounded form `NBDA_type=1`, $A=1-s$. Otherwise, $A=1$, which is now the accepted \"default\" in much of the literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A_param():\n",
    "    if asocial_learning:\n",
    "        if NBDA_type==\"1\":\n",
    "            assert 0 <= s_param <=1, \"s parameter must take a value between 0 and 1 for this type of NBDA.\"\n",
    "            asocial_learning_func=1-s_param\n",
    "        else:\n",
    "            asocial_learning_func=1\n",
    "    else:\n",
    "        asocial_learning_func=0\n",
    "\n",
    "    return asocial_learning_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networks <a class=\"anchor\" id=\"networks\"></a>\n",
    "\n",
    "Key to these simulations is that agents are situated within a social network. This is generated using the following function, which employs the `networkx` library. This is a powerful library with many pre-made network generation functions available. The full list at time of publication is [available here](https://networkx.org/documentation/stable/reference/generators.html). The type of graph is defined using the simulation parameter `graph_type`. We have included options for complete networks and random Erdős-Rényi graphs, and the import of a custom graph from an adjacency list. However, this function can be easily modified with other options.\n",
    "\n",
    "Once the graph is created, each node is populated with an instance of the agent class as it's `data` key (each node is defined as a dictionary in networkx). Once an agent is created, a dice is rolled to see whether they will be initially knowledgable, governed by the simulation variable `intial_knowledgable_prop`. The agent is then preprogrammed with knowledge of behavior \"a\", an intial attraction score, and a memory of socially observing the behavior. How this is done is totally up to the design and purpose of the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_network(graph_type):\n",
    "    if graph_type == \"complete\":\n",
    "        G = nx.complete_graph(N)\n",
    "    elif graph_type == \"random_regular\":\n",
    "        G = nx.random_regular_graph(int(N/2), N, seed=None)\n",
    "    elif graph_type == \"random_erdos\":\n",
    "        G = nx.gnp_random_graph(N, 0.5333, seed=None, directed=False)\n",
    "        while not nx.is_connected(G):\n",
    "            G = nx.gnp_random_graph(N, 0.5333, seed=None, directed=False)\n",
    "    elif graph_type == \"random_barabasi\":\n",
    "        G = nx.barabasi_albert_graph(N, 4, seed=None)\n",
    "    elif graph_type == \"random_small_world\":\n",
    "        G = nx.connected_watts_strogatz_graph(N, int(N/2), 0, tries=200, seed=None)\n",
    "    elif graph_type == \"custom_adj_list\":\n",
    "        G = nx.read_adjlist(custom_adj_list_filename)\n",
    "    else:\n",
    "        print(\"Incorrect graph name!\")\n",
    "\n",
    "    random_agent = random.randint(0, N-1)\n",
    "    for x in list(G.nodes()):\n",
    "        G.nodes[x]['data'] = agent(x)\n",
    "        if random.random() <= initial_knowledgable_prop:\n",
    "            G.nodes[x]['data'].knowledge[\"a\"] = {\"a_mat\": 0,\"i_mat\": 0,\"s_mat\":0,\"p_mat\":0}\n",
    "            #G.nodes[x]['data'].A_mat_update(\"a\")\n",
    "            G.nodes[x]['data'].I_mat_update()\n",
    "            G.nodes[x]['data'].S_mat_update()\n",
    "            G.nodes[x]['data'].P_mat_update()\n",
    "            G.nodes[x]['data'].naive=False\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation <a class=\"anchor\" id=\"simulation\"></a>\n",
    "\n",
    "We will now cover the structure of the simulation, and review the various helper functions it requires.\n",
    "\n",
    "1. We assign the simulation a unique number\n",
    "2. We create a csv to hold the data output\n",
    "3. We generate a unique network for the simulation and populate it with agents\n",
    "4. We begin a loop with one pass per timestep. In this loop:\n",
    "    1. A counter object is created to keep track of the frequencies of each behavior\n",
    "    2. A list of knowledgable agents is created. \n",
    "    3. Each knowledgable agent produces 1 behavior, which is added to the `beh_freqs` counter.\n",
    "    4. After all knowledgable agents have produced their behavior for the timestep, every agent in the population \n",
    "        * consolidates their short term social memory into long term memory\n",
    "        * prunes any individual memory that lays beyond the memory window\n",
    "        * prunes any social observation memory that lays beyond the memory window\n",
    "        * updates their social weight matrix and probability matrix\n",
    "    6. The number knowledgable of the novel behavior is counted.\n",
    "    7. All relevant data is written to the csv.\n",
    "    8. Agents update their knowledge states using the NBDA equations. Here they may acquire a new behavior to produce in the next time step.\n",
    "    9. Agents update their `exposure` variable, which is a count of how many time steps they have existed in the simulation\n",
    "    10. If there is turnover and the timestep is one in which turnover occurs, the oldest agents are replaced with new agents.\n",
    "5. When all timesteps have passed, the simulation ends.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation():\n",
    "    global master_sim_num\n",
    "    sim_num = master_sim_num\n",
    "    master_sim_num += 1\n",
    "    #create csv file with headers\n",
    "    file_path = \"../raw_data/_gtype_{}_sim_{}.csv\".format(graph_type, sim_num)\n",
    "\n",
    "    #if not isfile(file_path):\n",
    "    create_csv(file_path)\n",
    "\n",
    "    print(\"simulation {}\".format(sim_num))\n",
    "\n",
    "    np.random.seed() #reset seed for random num generation\n",
    "    G = generate_network(graph_type) #create network populated with agents\n",
    "\n",
    "    timestep=0\n",
    "    while True:\n",
    "\n",
    "        know_novel = [agent for agent in range(N) if \"b\" in G.nodes[agent][\"data\"].knowledge.keys()]\n",
    "        num_know_novel = len(know_novel)\n",
    "\n",
    "        if timestep%10==0:\n",
    "            agent_matrix_values = peek_inside(G)\n",
    "\n",
    "        #create counter object to keep track of frequencies of behaviors for this timestep\n",
    "        beh_freqs = Counter()\n",
    "        for behavior in all_behaviors.values():\n",
    "            beh_freqs[behavior.name] += 0\n",
    "\n",
    "        #create list of all knowledgable agents\n",
    "        knowledgable = [agent for agent in range(N) if G.nodes[agent][\"data\"].naive == False ]\n",
    "\n",
    "        #knowledgable individuals produce given number of behaviors beh_per_TS\n",
    "        for individual in knowledgable:\n",
    "            for interactions in range(beh_per_TS):\n",
    "                produced_behavior = production_event(G, individual)\n",
    "                beh_freqs[produced_behavior] += 1\n",
    "\n",
    "        #resolves internal counts to the memory window\n",
    "        for agent in range(N):\n",
    "            G.nodes[agent][\"data\"].prune_ind_memory()\n",
    "            G.nodes[agent][\"data\"].consolidate_social_memory()\n",
    "            G.nodes[agent][\"data\"].prune_social_memory()\n",
    "            G.nodes[agent][\"data\"].reset_temp_social_memory()\n",
    "            if not G.nodes[agent][\"data\"].naive:\n",
    "                G.nodes[agent][\"data\"].I_mat_update()\n",
    "                G.nodes[agent][\"data\"].S_mat_update()\n",
    "                G.nodes[agent][\"data\"].P_mat_update()\n",
    "\n",
    "        # write data and end sim when at full diffusion\n",
    "        if num_know_novel==N:\n",
    "            write_csv(file_path,sim_num,timestep,num_know_novel,beh_freqs,agent_matrix_values)\n",
    "            break\n",
    "\n",
    "        #write data\n",
    "        if timestep%10==0:\n",
    "            write_csv(file_path,sim_num,timestep,num_know_novel,beh_freqs,agent_matrix_values)\n",
    "\n",
    "        #run NBDA calculations\n",
    "        NBDA(G)\n",
    "        timestep+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions <a class=\"anchor\" id=\"helper-agent\"></a>\n",
    "\n",
    "`production_event` has the focal individual produce a behavior, and the focal individual's neighbors observe that behavioral production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def production_event(G, producer):\n",
    "    beh_production = G.nodes[producer][\"data\"].produceBehavior()\n",
    "\n",
    "    neighbors = [node for node in G.neighbors(producer)]\n",
    "    for neighbor in neighbors:\n",
    "        G.nodes[neighbor][\"data\"].observe(beh_production)\n",
    "    return beh_production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NBDA` simply loops through all agents and has them each call their `acquire_behavior` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NBDA(G):\n",
    "    for agent in list(G.nodes()):\n",
    "        G.nodes[agent][\"data\"].acquire_behavior(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions: extracting data <a class=\"anchor\" id=\"helper-extract\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv(file_path):\n",
    "    #create variable names for behaviors to be included in simulation\n",
    "    behavior_list = [\"behavior_{}\".format(behavior.name) for behavior in all_behaviors.values()]\n",
    "    labels=[\"A_mat\",\"I_mat\",\"S_mat\",\"P_mat\"]\n",
    "    agent_matrix_list = [\"behavior_{}_{}\".format(behavior.name,label) for behavior in all_behaviors.values() for label in labels]\n",
    "\n",
    "    #writes header for main data\n",
    "    with open(file_path,\"w\") as fout:\n",
    "        fout.write(\"sim, graph_type, pop_size, memory_window, NBDA_type, NBDA_basehazard, NBDA_s_param, NBDA_z_jt_type, NBDA_conformity, EWA_soc_info_weight, EWA_recent_payoff_weight, EWA_conformity, EWA_tau, timestep, num_know_novel, {}, {}\\n\".format(\",\".join(behavior_list),\",\".join(agent_matrix_list)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv(file_path,sim_num,timestep,num_know_novel,beh_freqs,agent_matrix_values):\n",
    "    behavior_counts = [str(beh) for beh in beh_freqs.values()]\n",
    "    #print(behavior_counts)\n",
    "    count_string = \",\".join(behavior_counts)\n",
    "\n",
    "    #writes header for main data\n",
    "    with open(file_path,\"a+\") as fout:\n",
    "        fout.write(\"{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "        sim_num,\n",
    "        graph_type,\n",
    "        N,\n",
    "        memory_window,\n",
    "        NBDA_type,\n",
    "        base_rate,\n",
    "        s_param,\n",
    "        z_jt_type,\n",
    "        f_SL,\n",
    "        sigma,\n",
    "        phi,\n",
    "        f_SL,\n",
    "        tau,\n",
    "        timestep,\n",
    "        num_know_novel,\n",
    "        count_string,\n",
    "        agent_matrix_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_values(agent, behavior,G):\n",
    "    #if the agent knows a behavior, return associated matrix values, otherwise return zeroes\n",
    "    if behavior in G.nodes[agent][\"data\"].knowledge.keys():\n",
    "        return [G.nodes[agent][\"data\"].knowledge[behavior][\"a_mat\"],\n",
    "        np.exp(G.nodes[agent][\"data\"].knowledge[behavior][\"i_mat\"]),\n",
    "        G.nodes[agent][\"data\"].knowledge[behavior][\"s_mat\"],\n",
    "        G.nodes[agent][\"data\"].knowledge[behavior][\"p_mat\"]]\n",
    "    else:\n",
    "        return [0,0,0,0]\n",
    "\n",
    "def peek_inside(G):\n",
    "    to_write_list = []\n",
    "    for behavior in all_behaviors.values():\n",
    "        #values for A_mat, I_mat, S_mat and P_mat\n",
    "        total_values = [0,0,0,0]\n",
    "        for agent in range(N):\n",
    "            agt_values = agent_values(agent,behavior.name,G)\n",
    "            #print(agt_values)\n",
    "            total_values = np.add(total_values,agt_values)\n",
    "        to_write_list += list(total_values)\n",
    "\n",
    "    agent_matrix_values_string = [str(v) for v in to_write_list]\n",
    "    amv_string = \",\".join(agent_matrix_values_string)\n",
    "    return amv_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the simulation <a class=\"anchor\" id=\"running\"></a>\n",
    "\n",
    "Finally, it's time to run the thing. This tutorial notebook has implemented a simpler version that runs one point in parameter space at a time, and doesn't capture full information from the agent's matrices. The full python version uses multi-threading to run these simulations much faster, across many points in parameter space.\n",
    "\n",
    "### Simulation parameters <a class=\"anchor\" id=\"sim_params\"></a>\n",
    "* `replicates` - number of replicates per point in parameter space\n",
    "* `beh_per_TS` - number of behaviors individuals perform per timestep\n",
    "* `master_sim_num` - begins at 0, this is incremented each simulation to give a unique sim ID\n",
    "* `directory_path` - where raw data will be output to\n",
    "* `new_directory_path` - where concatenated data will be output to at the end of all simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replicates=3\n",
    "master_sim_num=0\n",
    "directory_path=\"../raw_data\"\n",
    "new_directory_path=\"../concat_data/csvs\"\n",
    "behavior_names = [\"a\",\"b\"]\n",
    "behavior_payoffs = [1,1]\n",
    "beh_per_TS = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Population parameters <a class=\"anchor\" id=\"pop_params\"></a>\n",
    "\n",
    "* `N` - population size\n",
    "* `memory_window` - how far back the agent can remember (in time steps) the behaviors that other agents produce\n",
    "* `initial_knowledgable_prop` - proportion of population that's initially knowledgable\n",
    "* `graph_type` - type of network (random, complete, custom)\n",
    "* `custom_adj_list_filename` - file path for custom network from adjacency list\n",
    "* `turnover` - boolean indicating whether turnover occurs\n",
    "* `turnover_interval` - interval in timesteps between turnover events\n",
    "* `num_turnover` - how many agents should be replaced each turnover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 16 #population size\n",
    "initial_knowledgable_prop = .5 #initial proportion of knowledgable individuals in the population\n",
    "full_initial_weights = False\n",
    "graph_type = \"random_regular\" #\"random_regular\", \"random_erdos\",\"random_barabasi\",\"random_small_world\"\n",
    "custom_adj_list_filename = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EWA parameters <a class=\"anchor\" id=\"EWA_params\"></a>\n",
    "\n",
    "* `sigma` - social information bias\n",
    "* `phi` - recent payoff bias\n",
    "* `f_SI` - conformity exponent\n",
    "* `tau` - sensitivity to differences in attraction score, also called inverse exploration parameter\n",
    "* `memory_window` - how far back agents can remember in timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EWA Parameters\n",
    "sigma = 0.5 \n",
    "phi = 0.5 \n",
    "f_SI = 1 \n",
    "tau = 1 \n",
    "memory_window = 25\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NBDA parameters <a class=\"anchor\" id=\"NBDA_params\"></a>\n",
    "\n",
    "* `NBDA_type` - the form of NBDA, type 0 is the general unbounded form; 1: bounded, linked S and (1-S); 2: Proportional rule (Firth et al. 2020); 3: Conformity rule (Firth et al. 2020)\n",
    "* `f_SL` - if using NBDA_type 3, the conformity exponent used by NBDA\n",
    "* `s_param` - strength of social learning per unit of connection\n",
    "* `z_jt_type` - #\"binary\" or \"proportional\", FSSL requires proportional\n",
    "* `asocial_learning` - boolean value, True if asocial learning occurs\n",
    "* `base_rate` - the base hazard is the underlying rate of transmission in every timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBDA_type = 0 # 0: Unbounded general form; \n",
    "f_SL = 1\n",
    "s_param = 50 #s parameter from NBDA indicating strength of social learning per unit of connection\n",
    "z_jt_type = \"proportional\" #\"binary\" or \"proportional\", proportional assigns z_j(t) a number between [0,1] depending on how frequently the produced behavior in previous timestep\n",
    "asocial_learning = True #True or false, depending on if asocial learning occurs\n",
    "base_rate = .01 #the base hazard is the underlying rate in every timestep that the behavior could be acquired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary of all behaviors\n",
    "all_behaviors = dict((name, behavior(name,behavior_payoffs[i], base_rate)) for i,name in enumerate(behavior_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the sim <a class=\"anchor\" id=\"run_sim\"></a>\n",
    "\n",
    "Now that all of that is defined, we can run the simulation however many `replicates` we want. In the full version, each replicate is run in parallel using different threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(replicates):\n",
    "    simulation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data management <a class=\"anchor\" id=\"data_mgmt\"></a>\n",
    "\n",
    "Finally, we concatenate all the raw CSV outputs into one managable CSV file, and delete all of the raw data. Don't forget to change the name of the CSV the next time you run this! Otherwise the old data will be over-written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [pd.read_csv(join(directory_path,f)) for f in listdir(directory_path) if \".csv\" in f]\n",
    "df_concat = pd.concat(df_list)\n",
    "df_concat.to_csv(join(new_directory_path,\"concatenated_data.csv\"), index = False)\n",
    "for f in listdir(directory_path):\n",
    "    if \".csv\" in f:\n",
    "        remove(join(directory_path,f))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
